{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZCfn4XFGp6ox",
        "PftIpNFhqB8J"
      ],
      "authorship_tag": "ABX9TyOXIZPM8R8pb8lwOY87vM9u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sibat119/papers-review-code-impl/blob/main/word2vec/word2vec_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFDHHgk4fXuQ"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.functional as F\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDKg_FZQpxPU"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSFVw39j8u2Y"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "class Word2vecDataset(Dataset):\n",
        "  def __init__(self, datapath, window_size):\n",
        "      self.window_size = window_size\n",
        "      self.sentences_count = 0\n",
        "      self.input_file = open(datapath, encoding=\"utf8\")\n",
        "      self.corpus = self.get_corpus(datapath)\n",
        "      self.words = self.get_words(self.corpus)\n",
        "      self.vocab_to_int, self.int_to_vocab = self.create_lookup_tables(self.words)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.sentences_count\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      \n",
        "\n",
        "  @staticmethod\n",
        "  def collate(batches):\n",
        "      all_u = [u for batch in batches for u, _, _ in batch if len(batch) > 0]\n",
        "      all_v = [v for batch in batches for _, v, _ in batch if len(batch) > 0]\n",
        "      all_neg_v = [neg_v for batch in batches for _, _, neg_v in batch if len(batch) > 0]\n",
        "\n",
        "      return torch.LongTensor(all_u), torch.LongTensor(all_v), torch.LongTensor(all_neg_v)\n",
        "  def get_corpus(self, datapath):\n",
        "    corpus = ''\n",
        "    with open('/content/wiki_clean.json') as input_file:\n",
        "      data_file = json.load(input_file)\n",
        "      for x in data_file:\n",
        "        self.sentences_count += 1\n",
        "        corpus += (x[\"text\"].lower().strip())\n",
        "    print(len(corpus.strip()))\n",
        "    return corpus\n",
        "  \n",
        "  def get_words(self, corpus):\n",
        "    # Replace punctuation with tokens so we can use them in our model\n",
        "    text = text.lower()\n",
        "    text = text.replace('.', ' <PERIOD> ')\n",
        "    text = text.replace(',', ' <COMMA> ')\n",
        "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
        "    text = text.replace(';', ' <SEMICOLON> ')\n",
        "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
        "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
        "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
        "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
        "    text = text.replace('--', ' <HYPHENS> ')\n",
        "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
        "    # text = text.replace('\\n', ' <NEW_LINE> ')\n",
        "    text = text.replace(':', ' <COLON> ')\n",
        "    words = text.split()\n",
        "    \n",
        "    # Remove all words with  5 or fewer occurences\n",
        "    word_counts = Counter(words)\n",
        "    trimmed_words = [word for word in words if word_counts[word] > 2]\n",
        "\n",
        "    return trimmed_words\n",
        "\n",
        "  def create_lookup_tables(words):\n",
        "      \"\"\"\n",
        "      Create lookup tables for vocabulary\n",
        "      :param words: Input list of words\n",
        "      :return: Two dictionaries, vocab_to_int, int_to_vocab\n",
        "      \"\"\"\n",
        "      word_counts = Counter(words)\n",
        "      # sorting the words from most to least frequent in text occurrence\n",
        "      sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "      # create int_to_vocab dictionaries\n",
        "      int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
        "      vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
        "\n",
        "      return vocab_to_int, int_to_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PASUmgZp4A_l"
      },
      "source": [
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7xGuhOl5o4A",
        "outputId": "fd425de2-5701-41e8-f9f7-6041eac9cfa4"
      },
      "source": [
        "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
        "int_words = [vocab_to_int[word] for word in words]\n",
        "\n",
        "print(int_words[:30])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 9, 5, 6, 1, 0, 10, 5, 3, 11, 9, 7, 0, 8, 0, 4, 8, 5, 12, 3, 0, 12, 4, 1, 0, 4, 0, 3, 8, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYWq_Y_Pon7S",
        "outputId": "0a9a1d35-ed5c-49db-b99f-53dad4046e60"
      },
      "source": [
        "\n",
        "threshold = 1e-5\n",
        "word_counts = Counter(int_words)\n",
        "#print(list(word_counts.items())[0])  # dictionary of int_words, how many times they appear\n",
        "\n",
        "total_count = len(int_words)\n",
        "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
        "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
        "# discard some frequent words, according to the subsampling equation\n",
        "# create a new list of words for training\n",
        "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
        "\n",
        "print(train_words[:30])\n",
        "print(Counter(train_words))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "Counter()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qP5HMczpHfI"
      },
      "source": [
        "def get_context_words(words, idx, window_size=5):\n",
        "    ''' Get a list of words in a window around an index. '''\n",
        "    \n",
        "    # R = np.random.randint(1, window_size+1)\n",
        "    start = idx - window_size if (idx - window_size) > 0 else 0\n",
        "    stop = idx + window_size\n",
        "    target_words = words[start:idx] + words[idx+1:stop+1]\n",
        "    \n",
        "    return list(target_words)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj1wNKw-qztV"
      },
      "source": [
        "def get_batches(words, batch_size, window_size=5):\n",
        "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
        "    \n",
        "    n_batches = len(words)//batch_size\n",
        "    \n",
        "    # only full batches\n",
        "    words = words[:n_batches*batch_size]\n",
        "    \n",
        "    for idx in range(0, len(words), batch_size):\n",
        "        x, y = [], []\n",
        "        batch = words[idx:idx+batch_size]\n",
        "        for ii in range(len(batch)):\n",
        "            batch_x = batch[ii]\n",
        "            batch_y = get_context_words(batch, ii, window_size)\n",
        "            print(batch_y)\n",
        "            y.extend(batch_y)\n",
        "            x.extend([batch_x]*len(batch_y))\n",
        "        yield x, y"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1WegZJ7p5Vo"
      },
      "source": [
        "int_text = [i for i in range(200)]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rkC2KgUrHo5"
      },
      "source": [
        "x,y = next(get_batches(int_text, batch_size=40, window_size=5))\n",
        "\n",
        "print('x\\n', x)\n",
        "print('y\\n', y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLpriprIroAP"
      },
      "source": [
        "def neg_sampling(self, vocab):\n",
        "  NEG_SIZE = 1e6\n",
        "  neg_word_list = []\n",
        "  sorted_vocab = []\n",
        "  freq_sum = np.sum(vocab[word]['word_freq']**0.75 for word in vocab)\n",
        "  for word in vocab:\n",
        "      sorted_vocab.append((word, vocab[word]['word_freq']))\n",
        "  sorted_vocab.sort(key=lambda tup: tup[1], reverse=True)\n",
        "  for word in sorted_vocab:\n",
        "      neg_word_list.extend([word[0]] * int((word[1]**0.75 / freq_sum) * NEG_SIZE))\n",
        "  return neg_word_list"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwEmh0qLSDt5"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEhCIENFL8r-"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "class Word2vecDataset(Dataset):\n",
        "    def __init__(self, datapath, window_size):\n",
        "        self.data = data\n",
        "        self.window_size = window_size\n",
        "        self.input_file = open(data.inputFileName, encoding=\"utf8\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.sentences_count\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        while True:\n",
        "            line = self.input_file.readline()\n",
        "            if not line:\n",
        "                self.input_file.seek(0, 0)\n",
        "                line = self.input_file.readline()\n",
        "\n",
        "            if len(line) > 1:\n",
        "                words = line.split()\n",
        "\n",
        "                if len(words) > 1:\n",
        "                    word_ids = [self.data.word2id[w] for w in words if\n",
        "                                w in self.data.word2id and np.random.rand() < self.data.discards[self.data.word2id[w]]]\n",
        "\n",
        "                    boundary = np.random.randint(1, self.window_size)\n",
        "                    return [(u, v, self.data.getNegatives(v, 5)) for i, u in enumerate(word_ids) for j, v in\n",
        "                            enumerate(word_ids[max(i - boundary, 0):i + boundary]) if u != v]\n",
        "\n",
        "    @staticmethod\n",
        "    def collate(batches):\n",
        "        all_u = [u for batch in batches for u, _, _ in batch if len(batch) > 0]\n",
        "        all_v = [v for batch in batches for _, v, _ in batch if len(batch) > 0]\n",
        "        all_neg_v = [neg_v for batch in batches for _, _, neg_v in batch if len(batch) > 0]\n",
        "\n",
        "        return torch.LongTensor(all_u), torch.LongTensor(all_v), torch.LongTensor(all_neg_v)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCfn4XFGp6ox"
      },
      "source": [
        "## **Define Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wncXsAjaqUlY"
      },
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_size, emb_dimension):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.word_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "        self.context_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "\n",
        "        initrange = 1.0 / self.emb_dimension\n",
        "        init.uniform_(self.word_embeddings.weight.data, -initrange, initrange)\n",
        "        init.constant_(self.context_embeddings.weight.data, 0)\n",
        "\n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        emb_u = self.word_embeddings(pos_u)\n",
        "        emb_v = self.context_embeddings(pos_v)\n",
        "        emb_neg_v = self.context_embeddings(neg_v)\n",
        "\n",
        "        score = torch.sum(torch.mul(emb_u, emb_v), dim=1)\n",
        "        score = torch.clamp(score, max=10, min=-10)\n",
        "        score = -F.logsigmoid(score)\n",
        "\n",
        "        neg_score = torch.bmm(emb_neg_v, emb_u.unsqueeze(2)).squeeze()\n",
        "        neg_score = torch.clamp(neg_score, max=10, min=-10)\n",
        "        neg_score = -torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
        "\n",
        "        return torch.mean(score + neg_score)\n",
        "\n",
        "    def save_embedding(self, id2word, file_name):\n",
        "        embedding = self.word_embeddings.weight.cpu().data.numpy()\n",
        "        with open(file_name, 'w') as f:\n",
        "            f.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
        "            for wid, w in id2word.items():\n",
        "                e = ' '.join(map(lambda x: str(x), embedding[wid]))\n",
        "                f.write('%s %s\\n' % (w, e))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV9CfmDntUu2"
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PftIpNFhqB8J"
      },
      "source": [
        "## **Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUyq3l8-qWkm"
      },
      "source": [
        "def train(skip_gram_model, dataloader, initial_lr=1e-5, iterations=3, device='cpu'):\n",
        "  for iteration in range(iterations):\n",
        "\n",
        "      print(\"\\n\\n\\nIteration: \" + str(iteration + 1))\n",
        "      optimizer = optim.SparseAdam(skip_gram_model.parameters(), lr=initial_lr)\n",
        "      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(dataloader))\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, sample_batched in enumerate(tqdm(dataloader)):\n",
        "\n",
        "          if len(sample_batched[0]) > 1:\n",
        "              pos_u = sample_batched[0].to(device)\n",
        "              pos_v = sample_batched[1].to(device)\n",
        "              neg_v = sample_batched[2].to(device)\n",
        "\n",
        "              scheduler.step()\n",
        "              optimizer.zero_grad()\n",
        "              loss = skip_gram_model.forward(pos_u, pos_v, neg_v)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "              running_loss = running_loss * 0.9 + loss.item() * 0.1\n",
        "              if i > 0 and i % 500 == 0:\n",
        "                  print(\" Loss: \" + str(running_loss))\n",
        "      # skip_gram_model.save_embedding(id2word, output_file_name)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR4Q2cN6qJmE"
      },
      "source": [
        "## **Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEg94x1QqYA7"
      },
      "source": [
        "skip_gram_model = SkipGramModel(len(vocab_to_int), 50)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJtcVndi4ygq"
      },
      "source": [
        "dataset = Word2vecDataset(self.data, window_size)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=dataset.collate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K-ROBxv4EDd"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train(skip_gram_model=skip_gram_model, dataloader=dataloader, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsdroAz7qOX3"
      },
      "source": [
        "## **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6NQl0Q_qZmR"
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    }
  ]
}